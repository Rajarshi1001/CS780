{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT_39GASkK4g",
        "outputId": "b7b5091c-e115-4b22-d538-f4ce192fbe76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.9.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium\n",
        "from gymnasium import spaces, utils\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from operator import add\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "EJQ02aJdkQgD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Random Maze Environment using the Gymnasium.Env class"
      ],
      "metadata": {
        "id": "89fGzumsynkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomMazeEnv(gymnasium.Env):\n",
        "\n",
        "    metadata = {\"render_modes\" : [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
        "    # Define constants for clearer code\n",
        "    LEFT = 0\n",
        "    DOWN = 3\n",
        "    RIGHT = 2\n",
        "    UP = 1\n",
        "\n",
        "    def __init__(self, render = None, seed = None):\n",
        "        super(RandomMazeEnv, self).__init__()\n",
        "\n",
        "        p = 0.8\n",
        "        self.grid_size = 3 * 4\n",
        "        # Number of discrete actions, we have four: left, down, right, up\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        self.observation_space = spaces.Discrete(12)\n",
        "        self.P = {\n",
        "            0: {\n",
        "                0: [\n",
        "                    [p + (1-p)/2, 0, -0.04, False],\n",
        "                    [(1-p)/2, 4, -0.04, False]\n",
        "                ],\n",
        "                3: [\n",
        "                    [(1-p)/2, 0, -0.04, False],\n",
        "                    [p, 4, -0.04, False],\n",
        "                    [(1-p)/2, 1, -0.04, False]\n",
        "                ],\n",
        "                2: [\n",
        "                    [(1-p)/2, 4, -0.04, False],\n",
        "                    [p, 1, -0.04, False],\n",
        "                    [(1-p)/2, 0, -0.04, False]\n",
        "                ],\n",
        "                1: [\n",
        "                    [(1-p)/2, 1, -0.04, False],\n",
        "                    [p + (1-p)/2, 0, -0.04, False],\n",
        "                ]\n",
        "            },\n",
        "            1: {\n",
        "                0: [\n",
        "                    [(1-p), 1, -0.04, False],\n",
        "                    [p, 0, -0.04, False],\n",
        "                ],\n",
        "                3: [\n",
        "                    [(1-p)/2, 0, -0.04, False],\n",
        "                    [p, 1, -0.04, False],\n",
        "                    [(1-p)/2, 2, -0.04, False]\n",
        "                ],\n",
        "                2: [\n",
        "                    [(1-p), 1, -0.04, False],\n",
        "                    [p, 2, -0.04, False],\n",
        "                ],\n",
        "                1: [\n",
        "                    [(1-p)/2, 2, -0.04, False],\n",
        "                    [p, 1, -0.04, False],\n",
        "                    [(1-p)/2, 0, -0.04, False]\n",
        "                ]\n",
        "            },\n",
        "            2: {\n",
        "                0: [\n",
        "                    [(1-p)/2, 2, -0.04, False],\n",
        "                    [p, 1, -0.04, False],\n",
        "                    [(1-p)/2, 6, -0.04, False]\n",
        "                ],\n",
        "                3: [\n",
        "                    [(1-p)/2, 1, -0.04, False],\n",
        "                    [p, 6, -0.04, False],\n",
        "                    [(1-p)/2, 3, 1.0, True]\n",
        "                ],\n",
        "                2: [\n",
        "                    [(1-p)/2, 6, -0.04, False],\n",
        "                    [p, 3, 1.0, True],\n",
        "                    [(1-p)/2, 2, -0.04, False]\n",
        "                ],\n",
        "                1: [\n",
        "                    [(1-p)/2, 3, 1.0, True],\n",
        "                    [p, 2, -0.04, False],\n",
        "                    [(1-p)/2, 1, -0.04, False]\n",
        "                ]\n",
        "            },\n",
        "            3: {\n",
        "                0: [\n",
        "                    [1.0, 3, 0.0, True]\n",
        "                ],\n",
        "                3: [\n",
        "                    [1.0, 3, 0.0, True]\n",
        "                ],\n",
        "                2: [\n",
        "                    [1.0, 3, 0.0, True]\n",
        "                ],\n",
        "                1: [\n",
        "                    [1.0, 3, 0.0, True]\n",
        "                ]\n",
        "            },\n",
        "            4: {\n",
        "                0: [\n",
        "                    [(1-p)/2, 0, -0.04, False],\n",
        "                    [p, 4, -0.04, False],\n",
        "                    [(1-p)/2, 8, -0.04, False]\n",
        "                ],\n",
        "                3: [\n",
        "                    [(1-p), 4, -0.04, False],\n",
        "                    [p, 8, -0.04, False],\n",
        "                ],\n",
        "                2: [\n",
        "                    [(1-p)/2, 8, -0.04, False],\n",
        "                    [p, 4, -0.04, False],\n",
        "                    [(1-p)/2, 0, -0.04, False]\n",
        "                ],\n",
        "                1: [\n",
        "                    [(1-p), 4, -0.04, False],\n",
        "                    [p, 0, -0.04, False],\n",
        "                ]\n",
        "            },\n",
        "            5: {\n",
        "                0: [\n",
        "                    [1.0, 5, 0.0, True]\n",
        "                ],\n",
        "                3: [\n",
        "                    [1.0, 5, 0.0, True]\n",
        "                ],\n",
        "                2: [\n",
        "                    [1.0, 5, 0.0, True]\n",
        "                ],\n",
        "                1: [\n",
        "                    [1.0, 5, 0.0, True]\n",
        "                ]\n",
        "            },\n",
        "            6: {\n",
        "                0: [\n",
        "                    [(1-p)/2, 2, -0.04, False],\n",
        "                    [p, 6, -0.04, False],\n",
        "                    [(1-p)/2, 10, -0.04, False]\n",
        "                ],\n",
        "                3: [\n",
        "                    [(1-p)/2, 6, -0.04, False],\n",
        "                    [p, 10, -0.04, False],\n",
        "                    [(1-p)/2, 7, -1.0, True]\n",
        "                ],\n",
        "                2: [\n",
        "                    [(1-p)/2, 10, -0.04, False],\n",
        "                    [p, 7, -1.0, True],\n",
        "                    [(1-p)/2, 2, -0.04, False]\n",
        "                ],\n",
        "                1: [\n",
        "                    [(1-p)/2, 7, -1.0, True],\n",
        "                    [p, 2, -0.04, False],\n",
        "                    [(1-p)/2, 6, -0.04, False]\n",
        "                ]\n",
        "            },\n",
        "            7: {\n",
        "                0: [\n",
        "                    [1.0, 7, 0.0, True]\n",
        "                ],\n",
        "                3: [\n",
        "                    [1.0, 7, 0.0, True]\n",
        "                ],\n",
        "                2: [\n",
        "                    [1.0, 7, 0.0, True]\n",
        "                ],\n",
        "                1: [\n",
        "                    [1.0, 7, 0.0, True]\n",
        "                ]\n",
        "            },\n",
        "            8: {\n",
        "                0: [\n",
        "                    [(1-p)/2, 4, -0.04, False],\n",
        "                    [p + (1-p)/2, 8, -0.04, False]\n",
        "                ],\n",
        "                3: [\n",
        "                    [p + (1-p)/2, 8, -0.04, False],\n",
        "                    [(1-p)/2, 9, -0.04, False]\n",
        "                ],\n",
        "                2: [\n",
        "                    [(1-p)/2, 8, -0.04, False],\n",
        "                    [p, 9, -0.04, False],\n",
        "                    [(1-p)/2, 4, -0.04, False]\n",
        "                ],\n",
        "                1: [\n",
        "                    [(1-p)/2, 9, -0.04, False],\n",
        "                    [p, 4, -0.04, False],\n",
        "                    [(1-p)/2, 8, -0.04, False]\n",
        "                ]\n",
        "            },\n",
        "            9: {\n",
        "                0: [\n",
        "                    [(1-p), 9, -0.04, False],\n",
        "                    [p, 8, -0.04, False]\n",
        "                ],\n",
        "                3: [\n",
        "                    [(1-p)/2, 8, -0.04, False],\n",
        "                    [p, 9, -0.04, False],\n",
        "                    [(1-p)/2, 10, -0.04, False]\n",
        "                ],\n",
        "                2: [\n",
        "                    [(1-p), 9, -0.04, False],\n",
        "                    [p, 10, -0.04, False]\n",
        "                ],\n",
        "                1: [\n",
        "                    [(1-p)/2, 10, -0.04, False],\n",
        "                    [p, 9, -0.04, False],\n",
        "                    [(1-p)/2, 8, -0.04, False]\n",
        "                ]\n",
        "            },\n",
        "            10: {\n",
        "                0: [\n",
        "                    [(1-p)/2, 6, -0.04, False],\n",
        "                    [p, 9, -0.04, False],\n",
        "                    [(1-p)/2, 10, -0.04, False]\n",
        "                ],\n",
        "                3: [\n",
        "                    [(1-p)/2, 9, -0.04, False],\n",
        "                    [p, 10, -0.04, False],\n",
        "                    [(1-p)/2, 11, -0.04, False]\n",
        "                ],\n",
        "                2: [\n",
        "                    [(1-p)/2, 10, -0.04, False],\n",
        "                    [p, 11, -0.04, False],\n",
        "                    [(1-p)/2, 6, -0.04, False]\n",
        "                ],\n",
        "                1: [\n",
        "                    [(1-p)/2, 11, -0.04, False],\n",
        "                    [p, 6, -0.04, False],\n",
        "                    [(1-p)/2, 9, -0.04, False]\n",
        "                ]\n",
        "            },\n",
        "            11: {\n",
        "                0: [\n",
        "                    [(1-p)/2, 7, -1.0, True],\n",
        "                    [p, 10, -0.04, False],\n",
        "                    [(1-p)/2, 11, -0.04, False]\n",
        "                ],\n",
        "                3: [\n",
        "                    [(1-p)/2, 10, -0.04, False],\n",
        "                    [p + (1-p)/2, 11, -0.04, False]\n",
        "                ],\n",
        "                2: [\n",
        "                    [p + (1-p)/2, 11, -0.04, False],\n",
        "                    [(1-p)/2, 7, -1.0, True]\n",
        "                ],\n",
        "                1: [\n",
        "                    [(1-p)/2, 11, -0.04, False],\n",
        "                    [p, 7, -1.0, True],\n",
        "                    [(1-p)/2, 10, -0.04, False]\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "        self.seed(seed)\n",
        "        self.start_state = 8\n",
        "        self.state = self.start_state\n",
        "        self.gamma = 0.99 # discount factor\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = gymnasium.utils.seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, action):\n",
        "        transitions = self.P[self.state][action]\n",
        "        i = self.np_random.choice(len(transitions), p=[t[0] for t in transitions])\n",
        "        prob, next_state, reward, done = transitions[i]\n",
        "        self.state = next_state\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "        # moving to the next state\n",
        "        self.state = next_state\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "    def reset(self, seed = None):\n",
        "        self.seed(seed)\n",
        "        # Reset the state of the environment to an initial state\n",
        "        self.state = self.start_state\n",
        "        return self.state\n",
        "\n",
        "    def close(self):\n",
        "         pass"
      ],
      "metadata": {
        "id": "ybKVJVcnkQiW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the `policyEvaluation()`, `policyImprovement()` and `policyIteration()` algorithm for the MDP of the Random Maze Experiment"
      ],
      "metadata": {
        "id": "VCE8fnlJyriv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the policy evaluation condition\n",
        "def policyEvaluation(policy, P, gamma, theta):\n",
        "    values_old = np.zeros(len(P.keys()))\n",
        "    while 1:\n",
        "        values_new = np.zeros(len(P.keys()))\n",
        "        # iterating over all states and actions\n",
        "        for state in P:\n",
        "            for action in P[state]:\n",
        "                factor = 0\n",
        "                if action == policy[state]:\n",
        "                    factor = 1\n",
        "                temp = 0\n",
        "                for prob, next_state, reward, _ in P[state][action]:\n",
        "                    temp += prob * (reward + gamma * values_old[next_state])\n",
        "                values_new[state] += factor * temp\n",
        "\n",
        "        # checking termination condition\n",
        "        diff_values = [abs(v1 - v2) for v1, v2 in zip(values_old, values_new)]\n",
        "        if max(diff_values) < theta:\n",
        "            break\n",
        "        values_old = values_new\n",
        "\n",
        "    return values_new\n",
        "\n",
        "# implementing the policy iteration\n",
        "def policyImprovement(values, P, gamma):\n",
        "  q_values = np.zeros((len(P.keys()), 4))\n",
        "  new_policy = np.zeros(len(P.keys()), dtype=int)\n",
        "  for state in P:\n",
        "    for action in P[state]:\n",
        "      for prob, next_state, reward, _ in P[state][action]:\n",
        "          q_values[state][action] += prob * (reward + gamma * values[next_state])\n",
        "\n",
        "    new_policy[state] = np.argmax(q_values[state])\n",
        "\n",
        "  # Returning the updated policy\n",
        "  return new_policy\n",
        "\n",
        "# Implementing the policy iteration\n",
        "def policyIteration(P, gamma, theta):\n",
        "  values_final = np.zeros(len(P.keys()))\n",
        "  policy_adverserial = [2,2,2,3,1,2,3,1,2,3,3,2]\n",
        "\n",
        "  iterations = 0\n",
        "  while True:\n",
        "    values = policyEvaluation(policy_adverserial, P, gamma, theta)\n",
        "    policy_new = policyImprovement(values, P, gamma)\n",
        "    iterations += 1\n",
        "    # checking the termination condition\n",
        "    if np.array_equal(policy_adverserial, policy_new):\n",
        "      values_final = values\n",
        "      break\n",
        "\n",
        "    policy_adverserial = policy_new\n",
        "\n",
        "  return policy_new, values_final, iterations"
      ],
      "metadata": {
        "id": "AQmBOSAzpPZA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_seed = 123\n",
        "gamma = 0.99\n",
        "theta = 10e-10\n",
        "env = RandomMazeEnv(seed = global_seed)\n",
        "\n",
        "print(len(env.P.keys()))\n",
        "\n",
        "optimal_policy, optimal_values, iterations = policyIteration(env.P, gamma, theta)\n",
        "\n",
        "print(\"Optimal Value Function:\", optimal_values)\n",
        "print(\"Optimal Policy:\", optimal_policy)\n",
        "print(\"Iterations:\", iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzlAWbkikQkd",
        "outputId": "e974a84b-743e-4d11-dfc6-b65340012eba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "Optimal Value Function: [0.82442985 0.89286374 0.95464233 0.         0.76427487 0.\n",
            " 0.68820946 0.         0.69763948 0.63906542 0.60613373 0.38186228]\n",
            "Optimal Policy: [2 2 2 0 1 0 1 0 1 0 1 0]\n",
            "Iterations: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the `valueIteration()` algorithm"
      ],
      "metadata": {
        "id": "Cr0rIT-QuWSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemeting the value iteration algorithm\n",
        "def valueIteration(P, gamma, theta):\n",
        "\n",
        "    values = np.zeros(len(P.keys()))  # Initialize value function\n",
        "    pi = dict()  # Initialize policy\n",
        "    iterations = 0\n",
        "\n",
        "    while True:\n",
        "        iterations += 1\n",
        "        delta = 0\n",
        "        for state in P.keys():\n",
        "            values_state = values[state]\n",
        "            Q_state_action = np.zeros(4)\n",
        "            for action in P[state].keys():\n",
        "                for prob, next_state, reward, _ in P[state][action]:\n",
        "                    Q_state_action[action] += prob * (reward + gamma * values[next_state])\n",
        "            values[state] = np.max(Q_state_action)\n",
        "            delta = max(delta, abs(values_state - values[state]))\n",
        "\n",
        "        # Break out of the loop when delta is less than theta\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # Derive policy from value function\n",
        "    for state in P.keys():\n",
        "        Q_state_action = np.zeros(4)\n",
        "        for action in P[state].keys():\n",
        "            for prob, next_state, reward, _ in P[state][action]:\n",
        "                Q_state_action[action] += prob * (reward + gamma * values[next_state])\n",
        "        pi[state] = np.argmax(Q_state_action)  # Choose action that maximizes the action-value function\n",
        "\n",
        "    return pi, values, iterations\n"
      ],
      "metadata": {
        "id": "5FjgtKA0mD_T"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_seed = 123\n",
        "gamma = 0.99\n",
        "theta = 10e-10\n",
        "env = RandomMazeEnv(seed = global_seed)\n",
        "\n",
        "print(len(env.P.keys()))\n",
        "\n",
        "optimal_policy, optimal_values, iterations = valueIteration(env.P, gamma, theta)\n",
        "\n",
        "print(\"Optimal Value Function:\", optimal_values)\n",
        "print(\"Optimal Policy:\", optimal_policy)\n",
        "print(\"Iterations:\", iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaHCqkvMkQpQ",
        "outputId": "e11b3588-cadd-4a65-f862-acd44182acad"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "Optimal Value Function: [0.82442985 0.89286374 0.95464233 0.         0.76427487 0.\n",
            " 0.68820946 0.         0.69763948 0.63906542 0.60613373 0.38186228]\n",
            "Optimal Policy: {0: 2, 1: 2, 2: 2, 3: 0, 4: 1, 5: 0, 6: 1, 7: 0, 8: 1, 9: 0, 10: 1, 11: 0}\n",
            "Iterations: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H6K5Y-WAkQu4"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}